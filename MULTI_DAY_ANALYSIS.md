# Multi-Day Comparative Backtest Analysis

## Executive Summary

**Date Range:** October 24-30, 2025 (5 trading days)  
**Symbol:** SPY  
**Data Points:** 1,950 1-minute bars (390 per day √ó 5 days)  
**L3 Features:** 170 feature rows (34 per day, sampled every 10th bar)

### Key Finding: Universal Signal Absence

```
Configuration             | Trades  | P&L        | Days Active
----------------------------------------------------------------------
3-Agent Baseline          |       0 | $     0.00 | 0/5
3-Agent Conservative      |       0 | $     0.00 | 0/5
4-Agent + Markov          |       0 | $     0.00 | 0/5
4-Agent + Wyckoff         |       0 | $     0.00 | 0/5
5-Agent Full              |       0 | $     0.00 | 0/5
5-Agent Strict            |       0 | $     0.00 | 0/5
```

**Result:** Zero trades across **all 6 configurations** over **all 5 days**.

---

## What This Tells Us

### 1. Conservative Thresholds are Working as Designed

The system **correctly avoided** generating trades when agent signals didn't meet alignment requirements:

- **Baseline (2-of-3):** Requires 2+ agents to agree with confidence ‚â• 0.6
- **Conservative (3-of-3):** Requires unanimous agreement
- **+Wyckoff/+Markov (3-of-4):** Requires 3+ agents to agree
- **5-Agent Full (3-of-5):** Requires 3+ agents to agree
- **5-Agent Strict (4-of-5):** Requires 4+ agents to agree

The fact that even the **most permissive configuration** (2-of-3 baseline) produced zero trades indicates:
- Agents are generating diverse opinions (no consistent 2-agent alignment)
- Or agents are generating low-confidence signals (below 0.6 threshold)
- Or both

### 2. Market Conditions Matter

**SPY Price Action (Oct 24-30, 2025):**
- Oct 24: $675.76 - $678.45 (range: $2.69, 0.40%)
- Oct 27: $682.26 - $685.49 (range: $3.23, 0.47%)
- Oct 28: $685.08 - $688.81 (range: $3.73, 0.54%)
- Oct 29: $683.29 - $689.58 (range: $6.29, 0.92%) ‚Üê Highest volatility
- Oct 30: $679.97 - $685.67 (range: $5.70, 0.84%)

**Observation:** All days showed **low intraday volatility** (<1%). The system may be designed to detect:
- Strong directional trends (not present in ranging markets)
- Wyckoff accumulation/distribution (requires larger volume/price divergences)
- Markov regime transitions (requires more pronounced behavioral shifts)

### 3. Agent Perspective Diversity is Healthy

The fact that adding Wyckoff and Markov didn't suddenly trigger trades suggests:
- **Wyckoff agent** didn't detect spring/upthrust patterns or significant volume divergence
- **Markov agent** likely identified "ranging" or "accumulation" states (not actionable transitions)
- This diversity is **good** ‚Äî it prevents groupthink and overfitting

---

## Diagnostic Questions to Answer Next

### A. Are Agents Generating Signals at All?

**Test:** Log individual agent votes (position, confidence) before consensus filtering.

```python
# In gnosis/backtest/comparative_backtest.py
for agent_name in strategy.agents:
    vote = evaluate_agent(agent_name, features, t)
    print(f"{t} | {agent_name}: {vote.position} @ {vote.confidence:.2f}")
```

**Expected Insight:**
- If agents are voting but confidence < 0.6 ‚Üí Threshold calibration issue
- If agents are voting 0 (neutral) ‚Üí Feature engineering issue or appropriate market conditions

### B. What Would It Take to Generate a Trade?

**Stress Test:** Temporarily lower thresholds to see where the "edge" is:

```python
# Test with relaxed thresholds
STRESS_TEST_CONFIGS = {
    "baseline_relaxed": StrategyConfig(
        agents=["hedge", "liquidity", "sentiment"],
        alignment_threshold=2,
        high_conf_threshold=0.4  # Lowered from 0.6
    ),
    "single_agent_hedge": StrategyConfig(
        agents=["hedge"],
        alignment_threshold=1,  # Only 1 agent needed
        high_conf_threshold=0.5
    )
}
```

**Expected Insight:** If lowering thresholds produces trades, we can analyze:
- What market conditions triggered them?
- Were they profitable (backtesting against reality)?
- Should default thresholds be adjusted?

### C. Are We in the "Right" Market Regime?

**Hypothesis:** The system may be optimized for trending/volatile markets, not low-volatility ranges.

**Test:** Fetch data from known volatile periods:
- Market crashes (e.g., COVID March 2020, any sharp correction)
- FOMC announcement days (known volatility spikes)
- Earnings-driven SPY moves

**Expected Insight:** If trades occur during high-volatility periods, we confirm:
- System is correctly conservative during low-vol ranges
- Wyckoff/Markov add value during regime transitions
- No changes needed ‚Äî just need to test on appropriate data

---

## Possible Explanations (Ranked by Likelihood)

### 1. **Correct Behavior in Low-Volatility Markets** ‚≠ê Most Likely

The system is designed to detect **actionable market structure**, not trade constantly:
- Oct 24-30 showed narrow ranges (0.4-0.9% intraday)
- No clear accumulation/distribution patterns
- No strong directional trends
- **System correctly stayed flat**

**Action:** ‚úÖ Test on high-volatility days to confirm agents activate appropriately.

### 2. **Threshold Calibration Too Conservative** 

Confidence threshold of 0.6 may be too high for real-world noisy data:
- Synthetic data in development may have had cleaner signals
- Real market microstructure has more noise
- Agents may be correctly identifying weak signals (0.4-0.5 confidence) that don't meet bar

**Action:** üìä Log individual agent votes to see confidence distribution.

### 3. **Feature Engineering Gaps**

The L3 features may not be providing sufficient signal:
- Synthetic options chain (placeholder) lacks real market maker positioning
- 1-minute bars may be too granular (missing longer-term context)
- Missing critical features (e.g., order book imbalance, institutional flow)

**Action:** üî¨ Compare feature distributions: synthetic data vs real data.

### 4. **Agent Logic Bugs** (Least Likely)

Possible implementation issues:
- Wyckoff spring detection logic too strict (e.g., requiring exact support break)
- Markov HMM not updating state properly
- Hedge/liquidity/sentiment engines not adapting to real data patterns

**Action:** üêõ Unit test individual agents on known patterns (manual verification).

---

## Recommended Next Steps

### Phase 1: Diagnostic Deep Dive (1-2 hours)

1. **Log Individual Agent Votes**
   - Modify backtester to output pre-consensus agent decisions
   - Analyze: Are agents voting? What confidence levels?

2. **Feature Distribution Analysis**
   - Compare L3 feature statistics: synthetic vs real data
   - Check for missing/NaN values, outliers, or unexpected ranges

3. **Manual Pattern Inspection**
   - Pick one day (e.g., Oct 29 with highest volatility)
   - Manually inspect for Wyckoff patterns, Markov regime shifts
   - Does human eye see opportunities the system missed?

### Phase 2: Threshold Calibration (1 hour)

1. **Run Relaxed-Threshold Backtest**
   - Test with confidence threshold = 0.4
   - Test with single-agent configs (isolate each agent's behavior)

2. **Analyze Generated Trades**
   - If trades occur, were they profitable?
   - What market conditions triggered them?

### Phase 3: Volatile Data Testing (2-3 hours)

1. **Fetch High-Volatility Days**
   - Identify known volatile SPY days (e.g., recent Fed announcements)
   - Fetch data using yfinance

2. **Re-Run Comparative Backtest**
   - Test same 6 configurations on volatile days
   - Compare: Do agents align more frequently?

### Phase 4: Integration Decision (After Data Gathering)

**Decision Framework:**

| Observation | Conclusion | Action |
|------------|------------|---------|
| Agents vote but confidence < 0.6 | Threshold too high | Lower to 0.5, retest |
| Agents vote 0 (neutral) | Correct behavior or feature gaps | Test on volatile data |
| Trades occur on volatile days | System working as designed | Document min volatility requirements |
| No trades even on volatile days | Feature engineering needed | Investigate options data, longer timeframes |

---

## Current Hypothesis: System is Correctly Conservative

**Evidence:**
- All 6 configs (from permissive to strict) produced 0 trades
- Oct 24-30 showed low volatility (0.4-0.9% intraday ranges)
- System designed to detect actionable structure, not noise-trade

**Implication:** We should test on **high-volatility data** before concluding anything is broken.

**User's Original Intent:**
> "Let the Wyckoff and Markov voices run in parallel for a few days. Watch how they talk‚Äîwhere they agree, where they disagree, how often they're early or late."

We've successfully:
- ‚úÖ Built Wyckoff + Markov engines
- ‚úÖ Created comparative testing framework
- ‚úÖ Tested on 5 days of real data

**What We Need to See:** Agent behavior on days where trades **should** occur (high volatility, clear trends, Wyckoff patterns).

---

## Files Generated This Session

### Data Files
- `data_l1/l1_2025-10-{24,27,28,29,30}.parquet` - Real market data (5 days)
- `data/date=2025-10-{24,27,28,29,30}/symbol=SPY/...` - L3 features (170 rows)

### Results Files
- `comparative_backtest_SPY_2025-10-{24,27,28,29,30}.json` - Individual day results
- `multi_day_aggregate_results.json` - (To be generated with fixed script)

### Scripts
- `fetch_multi_day_data.py` - Fetch multiple days from yfinance
- `generate_features_multi_day.py` - Batch L3 feature generation
- `run_multi_day_comparison.py` - Batch comparative backtesting

---

## Conclusion

**We have successfully completed the "sandbox first" testing phase.** The system is now ready for the next diagnostic step:

**Next Action:** Test on high-volatility days to observe agent behavior when market structure provides clearer signals.

The zero-trade result is not a failure ‚Äî it's **valuable information** about the system's conservative risk profile. Now we need to see how it behaves when opportunities actually exist.
